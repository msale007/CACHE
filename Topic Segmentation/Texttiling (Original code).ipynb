{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70ec63b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "import re\n",
    "\n",
    "try:\n",
    "    import numpy\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "from nltk.tokenize.api import TokenizerI\n",
    "\n",
    "BLOCK_COMPARISON, VOCABULARY_INTRODUCTION = 0, 1\n",
    "LC, HC = 0, 1\n",
    "DEFAULT_SMOOTHING = [0]\n",
    "\n",
    "\n",
    "class TextTilingTokenizer(TokenizerI):\n",
    "    \"\"\"Tokenize a document into topical sections using the TextTiling algorithm.\n",
    "    This algorithm detects subtopic shifts based on the analysis of lexical\n",
    "    co-occurrence patterns.\n",
    "\n",
    "    The process starts by tokenizing the text into pseudosentences of\n",
    "    a fixed size w. Then, depending on the method used, similarity\n",
    "    scores are assigned at sentence gaps. The algorithm proceeds by\n",
    "    detecting the peak differences between these scores and marking\n",
    "    them as boundaries. The boundaries are normalized to the closest\n",
    "    paragraph break and the segmented text is returned.\n",
    "\n",
    "    :param w: Pseudosentence size\n",
    "    :type w: int\n",
    "    :param k: Size (in sentences) of the block used in the block comparison method\n",
    "    :type k: int\n",
    "    :param similarity_method: The method used for determining similarity scores:\n",
    "       `BLOCK_COMPARISON` (default) or `VOCABULARY_INTRODUCTION`.\n",
    "    :type similarity_method: constant\n",
    "    :param stopwords: A list of stopwords that are filtered out (defaults to NLTK's stopwords corpus)\n",
    "    :type stopwords: list(str)\n",
    "    :param smoothing_method: The method used for smoothing the score plot:\n",
    "      `DEFAULT_SMOOTHING` (default)\n",
    "    :type smoothing_method: constant\n",
    "    :param smoothing_width: The width of the window used by the smoothing method\n",
    "    :type smoothing_width: int\n",
    "    :param smoothing_rounds: The number of smoothing passes\n",
    "    :type smoothing_rounds: int\n",
    "    :param cutoff_policy: The policy used to determine the number of boundaries:\n",
    "      `HC` (default) or `LC`\n",
    "    :type cutoff_policy: constant\n",
    "\n",
    "    >>> from nltk.corpus import brown\n",
    "    >>> tt = TextTilingTokenizer(demo_mode=True)\n",
    "    >>> text = brown.raw()[:4000]\n",
    "    >>> s, ss, d, b = tt.tokenize(text)\n",
    "    >>> b\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        w=20,\n",
    "        k=10,\n",
    "        similarity_method=BLOCK_COMPARISON,\n",
    "        stopwords=None,\n",
    "        smoothing_method=DEFAULT_SMOOTHING,\n",
    "        smoothing_width=2,\n",
    "        smoothing_rounds=1,\n",
    "        cutoff_policy=HC,\n",
    "        demo_mode=False,\n",
    "    ):\n",
    "\n",
    "        if stopwords is None:\n",
    "            from nltk.corpus import stopwords\n",
    "\n",
    "            stopwords = stopwords.words(\"english\")\n",
    "        self.__dict__.update(locals())\n",
    "        del self.__dict__[\"self\"]\n",
    "\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        \"\"\"Return a tokenized copy of *text*, where each \"token\" represents\n",
    "        a separate topic.\"\"\"\n",
    "\n",
    "        lowercase_text = text.lower()\n",
    "        paragraph_breaks = self._mark_paragraph_breaks(text)\n",
    "        text_length = len(lowercase_text)\n",
    "\n",
    "        # Tokenization step starts here\n",
    "\n",
    "        # Remove punctuation\n",
    "        nopunct_text = \"\".join(\n",
    "            c for c in lowercase_text if re.match(r\"[a-z\\-' \\n\\t]\", c)\n",
    "        )\n",
    "        nopunct_par_breaks = self._mark_paragraph_breaks(nopunct_text)\n",
    "\n",
    "        tokseqs = self._divide_to_tokensequences(nopunct_text)\n",
    "\n",
    "        # The morphological stemming step mentioned in the TextTile\n",
    "        # paper is not implemented.  A comment in the original C\n",
    "        # implementation states that it offers no benefit to the\n",
    "        # process. It might be interesting to test the existing\n",
    "        # stemmers though.\n",
    "        # words = _stem_words(words)\n",
    "\n",
    "        # Filter stopwords\n",
    "        for ts in tokseqs:\n",
    "            ts.wrdindex_list = [\n",
    "                wi for wi in ts.wrdindex_list if wi[0] not in self.stopwords\n",
    "            ]\n",
    "\n",
    "        token_table = self._create_token_table(tokseqs, nopunct_par_breaks)\n",
    "        # End of the Tokenization step\n",
    "\n",
    "        # Lexical score determination\n",
    "        if self.similarity_method == BLOCK_COMPARISON:\n",
    "            gap_scores = self._block_comparison(tokseqs, token_table)\n",
    "        elif self.similarity_method == VOCABULARY_INTRODUCTION:\n",
    "            raise NotImplementedError(\"Vocabulary introduction not implemented\")\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Similarity method {self.similarity_method} not recognized\"\n",
    "            )\n",
    "\n",
    "        if self.smoothing_method == DEFAULT_SMOOTHING:\n",
    "            smooth_scores = self._smooth_scores(gap_scores)\n",
    "        else:\n",
    "            raise ValueError(f\"Smoothing method {self.smoothing_method} not recognized\")\n",
    "        # End of Lexical score Determination\n",
    "\n",
    "        # Boundary identification\n",
    "        depth_scores = self._depth_scores(smooth_scores)\n",
    "        segment_boundaries = self._identify_boundaries(depth_scores)\n",
    "\n",
    "        normalized_boundaries = self._normalize_boundaries(\n",
    "            text, segment_boundaries, paragraph_breaks\n",
    "        )\n",
    "        # End of Boundary Identification\n",
    "        segmented_text = []\n",
    "        prevb = 0\n",
    "\n",
    "        for b in normalized_boundaries:\n",
    "            if b == 0:\n",
    "                continue\n",
    "            segmented_text.append(text[prevb:b])\n",
    "            prevb = b\n",
    "\n",
    "        if prevb < text_length:  # append any text that may be remaining\n",
    "            segmented_text.append(text[prevb:])\n",
    "\n",
    "        if not segmented_text:\n",
    "            segmented_text = [text]\n",
    "\n",
    "        if self.demo_mode:\n",
    "            return gap_scores, smooth_scores, depth_scores, segment_boundaries\n",
    "        return segmented_text\n",
    "\n",
    "\n",
    "    def _block_comparison(self, tokseqs, token_table):\n",
    "        \"\"\"Implements the block comparison method\"\"\"\n",
    "\n",
    "        def blk_frq(tok, block):\n",
    "            ts_occs = filter(lambda o: o[0] in block, token_table[tok].ts_occurences)\n",
    "            freq = sum(tsocc[1] for tsocc in ts_occs)\n",
    "            return freq\n",
    "\n",
    "        gap_scores = []\n",
    "        numgaps = len(tokseqs) - 1\n",
    "\n",
    "        for curr_gap in range(numgaps):\n",
    "            score_dividend, score_divisor_b1, score_divisor_b2 = 0.0, 0.0, 0.0\n",
    "            score = 0.0\n",
    "            # adjust window size for boundary conditions\n",
    "            if curr_gap < self.k - 1:\n",
    "                window_size = curr_gap + 1\n",
    "            elif curr_gap > numgaps - self.k:\n",
    "                window_size = numgaps - curr_gap\n",
    "            else:\n",
    "                window_size = self.k\n",
    "\n",
    "            b1 = [ts.index for ts in tokseqs[curr_gap - window_size + 1 : curr_gap + 1]]\n",
    "            b2 = [ts.index for ts in tokseqs[curr_gap + 1 : curr_gap + window_size + 1]]\n",
    "\n",
    "            for t in token_table:\n",
    "                score_dividend += blk_frq(t, b1) * blk_frq(t, b2)\n",
    "                score_divisor_b1 += blk_frq(t, b1) ** 2\n",
    "                score_divisor_b2 += blk_frq(t, b2) ** 2\n",
    "            try:\n",
    "                score = score_dividend / math.sqrt(score_divisor_b1 * score_divisor_b2)\n",
    "            except ZeroDivisionError:\n",
    "                pass  # score += 0.0\n",
    "\n",
    "            gap_scores.append(score)\n",
    "\n",
    "        return gap_scores\n",
    "\n",
    "    def _smooth_scores(self, gap_scores):\n",
    "        \"Wraps the smooth function from the SciPy Cookbook\"\n",
    "        return list(\n",
    "            smooth(numpy.array(gap_scores[:]), window_len=self.smoothing_width + 1)\n",
    "        )\n",
    "\n",
    "    def _mark_paragraph_breaks(self, text):\n",
    "        \"\"\"Identifies indented text or line breaks as the beginning of\n",
    "        paragraphs\"\"\"\n",
    "        MIN_PARAGRAPH = 100\n",
    "        pattern = re.compile(\"[ \\t\\r\\f\\v]*\\n[ \\t\\r\\f\\v]*\\n[ \\t\\r\\f\\v]*\")\n",
    "        matches = pattern.finditer(text)\n",
    "\n",
    "        last_break = 0\n",
    "        pbreaks = [0]\n",
    "        for pb in matches:\n",
    "            if pb.start() - last_break < MIN_PARAGRAPH:\n",
    "                continue\n",
    "            else:\n",
    "                pbreaks.append(pb.start())\n",
    "                last_break = pb.start()\n",
    "\n",
    "        return pbreaks\n",
    "\n",
    "    def _divide_to_tokensequences(self, text):\n",
    "        \"Divides the text into pseudosentences of fixed size\"\n",
    "        w = self.w\n",
    "        wrdindex_list = []\n",
    "        matches = re.finditer(r\"\\w+\", text)\n",
    "        for match in matches:\n",
    "            wrdindex_list.append((match.group(), match.start()))\n",
    "        return [\n",
    "            TokenSequence(i / w, wrdindex_list[i : i + w])\n",
    "            for i in range(0, len(wrdindex_list), w)\n",
    "        ]\n",
    "\n",
    "    def _create_token_table(self, token_sequences, par_breaks):\n",
    "        \"Creates a table of TokenTableFields\"\n",
    "        token_table = {}\n",
    "        current_par = 0\n",
    "        current_tok_seq = 0\n",
    "        pb_iter = par_breaks.__iter__()\n",
    "        current_par_break = next(pb_iter)\n",
    "        if current_par_break == 0:\n",
    "            try:\n",
    "                current_par_break = next(pb_iter)  # skip break at 0\n",
    "            except StopIteration as e:\n",
    "                raise ValueError(\n",
    "                    \"No paragraph breaks were found(text too short perhaps?)\"\n",
    "                ) from e\n",
    "        for ts in token_sequences:\n",
    "            for word, index in ts.wrdindex_list:\n",
    "                try:\n",
    "                    while index > current_par_break:\n",
    "                        current_par_break = next(pb_iter)\n",
    "                        current_par += 1\n",
    "                except StopIteration:\n",
    "                    # hit bottom\n",
    "                    pass\n",
    "\n",
    "                if word in token_table:\n",
    "                    token_table[word].total_count += 1\n",
    "\n",
    "                    if token_table[word].last_par != current_par:\n",
    "                        token_table[word].last_par = current_par\n",
    "                        token_table[word].par_count += 1\n",
    "\n",
    "                    if token_table[word].last_tok_seq != current_tok_seq:\n",
    "                        token_table[word].last_tok_seq = current_tok_seq\n",
    "                        token_table[word].ts_occurences.append([current_tok_seq, 1])\n",
    "                    else:\n",
    "                        token_table[word].ts_occurences[-1][1] += 1\n",
    "                else:  # new word\n",
    "                    token_table[word] = TokenTableField(\n",
    "                        first_pos=index,\n",
    "                        ts_occurences=[[current_tok_seq, 1]],\n",
    "                        total_count=1,\n",
    "                        par_count=1,\n",
    "                        last_par=current_par,\n",
    "                        last_tok_seq=current_tok_seq,\n",
    "                    )\n",
    "\n",
    "            current_tok_seq += 1\n",
    "\n",
    "        return token_table\n",
    "\n",
    "    def _identify_boundaries(self, depth_scores):\n",
    "        \"\"\"Identifies boundaries at the peaks of similarity score\n",
    "        differences\"\"\"\n",
    "\n",
    "        boundaries = [0 for x in depth_scores]\n",
    "\n",
    "        avg = sum(depth_scores) / len(depth_scores)\n",
    "        stdev = numpy.std(depth_scores)\n",
    "\n",
    "        if self.cutoff_policy == LC:\n",
    "            cutoff = avg - stdev\n",
    "        else:\n",
    "            cutoff = avg - stdev / 2.0\n",
    "\n",
    "        depth_tuples = sorted(zip(depth_scores, range(len(depth_scores))))\n",
    "        depth_tuples.reverse()\n",
    "        hp = list(filter(lambda x: x[0] > cutoff, depth_tuples))\n",
    "\n",
    "        for dt in hp:\n",
    "            boundaries[dt[1]] = 1\n",
    "            for dt2 in hp:  # undo if there is a boundary close already\n",
    "                if (\n",
    "                    dt[1] != dt2[1]\n",
    "                    and abs(dt2[1] - dt[1]) < 4\n",
    "                    and boundaries[dt2[1]] == 1\n",
    "                ):\n",
    "                    boundaries[dt[1]] = 0\n",
    "        return boundaries\n",
    "\n",
    "    def _depth_scores(self, scores):\n",
    "        \"\"\"Calculates the depth of each gap, i.e. the average difference\n",
    "        between the left and right peaks and the gap's score\"\"\"\n",
    "\n",
    "        depth_scores = [0 for x in scores]\n",
    "        # clip boundaries: this holds on the rule of thumb(my thumb)\n",
    "        # that a section shouldn't be smaller than at least 2\n",
    "        # pseudosentences for small texts and around 5 for larger ones.\n",
    "\n",
    "        clip = min(max(len(scores) // 10, 2), 5)\n",
    "        index = clip\n",
    "\n",
    "        for gapscore in scores[clip:-clip]:\n",
    "            lpeak = gapscore\n",
    "            for score in scores[index::-1]:\n",
    "                if score >= lpeak:\n",
    "                    lpeak = score\n",
    "                else:\n",
    "                    break\n",
    "            rpeak = gapscore\n",
    "            for score in scores[index:]:\n",
    "                if score >= rpeak:\n",
    "                    rpeak = score\n",
    "                else:\n",
    "                    break\n",
    "            depth_scores[index] = lpeak + rpeak - 2 * gapscore\n",
    "            index += 1\n",
    "\n",
    "        return depth_scores\n",
    "\n",
    "    def _normalize_boundaries(self, text, boundaries, paragraph_breaks):\n",
    "        \"\"\"Normalize the boundaries identified to the original text's\n",
    "        paragraph breaks\"\"\"\n",
    "\n",
    "        norm_boundaries = []\n",
    "        char_count, word_count, gaps_seen = 0, 0, 0\n",
    "        seen_word = False\n",
    "\n",
    "        for char in text:\n",
    "            char_count += 1\n",
    "            if char in \" \\t\\n\" and seen_word:\n",
    "                seen_word = False\n",
    "                word_count += 1\n",
    "            if char not in \" \\t\\n\" and not seen_word:\n",
    "                seen_word = True\n",
    "            if gaps_seen < len(boundaries) and word_count > (\n",
    "                max(gaps_seen * self.w, self.w)\n",
    "            ):\n",
    "                if boundaries[gaps_seen] == 1:\n",
    "                    # find closest paragraph break\n",
    "                    best_fit = len(text)\n",
    "                    for br in paragraph_breaks:\n",
    "                        if best_fit > abs(br - char_count):\n",
    "                            best_fit = abs(br - char_count)\n",
    "                            bestbr = br\n",
    "                        else:\n",
    "                            break\n",
    "                    if bestbr not in norm_boundaries:  # avoid duplicates\n",
    "                        norm_boundaries.append(bestbr)\n",
    "                gaps_seen += 1\n",
    "\n",
    "        return norm_boundaries\n",
    "\n",
    "\n",
    "\n",
    "class TokenTableField:\n",
    "    \"\"\"A field in the token table holding parameters for each token,\n",
    "    used later in the process\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        first_pos,\n",
    "        ts_occurences,\n",
    "        total_count=1,\n",
    "        par_count=1,\n",
    "        last_par=0,\n",
    "        last_tok_seq=None,\n",
    "    ):\n",
    "        self.__dict__.update(locals())\n",
    "        del self.__dict__[\"self\"]\n",
    "\n",
    "\n",
    "\n",
    "class TokenSequence:\n",
    "    \"A token list with its original length and its index\"\n",
    "\n",
    "    def __init__(self, index, wrdindex_list, original_length=None):\n",
    "        original_length = original_length or len(wrdindex_list)\n",
    "        self.__dict__.update(locals())\n",
    "        del self.__dict__[\"self\"]\n",
    "\n",
    "\n",
    "\n",
    "# Pasted from the SciPy cookbook: https://www.scipy.org/Cookbook/SignalSmooth\n",
    "def smooth(x, window_len=11, window=\"flat\"):\n",
    "    \"\"\"smooth the data using a window with requested size.\n",
    "\n",
    "    This method is based on the convolution of a scaled window with the signal.\n",
    "    The signal is prepared by introducing reflected copies of the signal\n",
    "    (with the window size) in both ends so that transient parts are minimized\n",
    "    in the beginning and end part of the output signal.\n",
    "\n",
    "    :param x: the input signal\n",
    "    :param window_len: the dimension of the smoothing window; should be an odd integer\n",
    "    :param window: the type of window from 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\n",
    "        flat window will produce a moving average smoothing.\n",
    "\n",
    "    :return: the smoothed signal\n",
    "\n",
    "    example::\n",
    "\n",
    "        t=linspace(-2,2,0.1)\n",
    "        x=sin(t)+randn(len(t))*0.1\n",
    "        y=smooth(x)\n",
    "\n",
    "    :see also: numpy.hanning, numpy.hamming, numpy.bartlett, numpy.blackman, numpy.convolve,\n",
    "        scipy.signal.lfilter\n",
    "\n",
    "    TODO: the window parameter could be the window itself if an array instead of a string\n",
    "    \"\"\"\n",
    "\n",
    "    if x.ndim != 1:\n",
    "        raise ValueError(\"smooth only accepts 1 dimension arrays.\")\n",
    "\n",
    "    if x.size < window_len:\n",
    "        raise ValueError(\"Input vector needs to be bigger than window size.\")\n",
    "\n",
    "    if window_len < 3:\n",
    "        return x\n",
    "\n",
    "    if window not in [\"flat\", \"hanning\", \"hamming\", \"bartlett\", \"blackman\"]:\n",
    "        raise ValueError(\n",
    "            \"Window is on of 'flat', 'hanning', 'hamming', 'bartlett', 'blackman'\"\n",
    "        )\n",
    "\n",
    "    s = numpy.r_[2 * x[0] - x[window_len:1:-1], x, 2 * x[-1] - x[-1:-window_len:-1]]\n",
    "\n",
    "    # print(len(s))\n",
    "    if window == \"flat\":  # moving average\n",
    "        w = numpy.ones(window_len, \"d\")\n",
    "    else:\n",
    "        w = eval(\"numpy.\" + window + \"(window_len)\")\n",
    "\n",
    "    y = numpy.convolve(w / w.sum(), s, mode=\"same\")\n",
    "\n",
    "    return y[window_len - 1 : -window_len + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f27d83c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\msalehi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
      "\n",
      "\n",
      "\tThe/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl Executive/jj-tl Committee/nn-tl ,/, which/wdt had/hvd over-all/jj charge/nn of/in the/at election/nn ,/, ``/`` deserves/vbz the/at praise/nn and/cc thanks/nns of/in the/at City/nn-tl of/in-tl Atlanta/np-tl ''/'' for/in the/at manner/nn in/in which/wdt the/at election/nn was/bedz conducted/vbn ./.\n",
      "\n",
      "\n",
      "\tThe/at September-October/np term/nn jury/nn had/hvd been/ben charged/vbn by/in Fulton/np-tl Superior/jj-tl Court/nn-tl Judge/nn-tl Durwood/np Pye/np to/to investigate/vb reports/nns of/in possible/jj ``/`` irregularities/nns ''/'' in/in the/at hard-fought/jj primary/nn which/wdt was/bedz won/vbn by/in Mayor-nominate/nn-tl Ivan/np Allen/np Jr./np ./.\n",
      "\n",
      "\n",
      "\t``/`` Only/rb a/at relative/jj handful/nn of/in such/jj reports/nns was/bedz received/vbn ''/'' ,/, the/at jury/nn said/vbd ,/, ``/`` considering/in the/at widespread/jj interest/nn in/in the/at election/nn ,/, the/at number/nn of/in voters/nns and/cc the/at size/nn of/in this/dt city/nn ''/'' ./.\n",
      "\n",
      "\n",
      "\tThe/at jury/nn said/vbd it/pps did/dod find/vb that/cs many/ap of/in Georgia's/np$ registration/nn and/cc election/nn laws/nns ``/`` are/ber outmoded/jj or/cc inadequate/jj and/cc often/rb ambiguous/jj ''/'' ./.\n",
      "\n",
      "\n",
      "\tIt/pps recommended/vbd that/cs Fulton/np legislators/nns act/vb ``/`` to/to have/hv these/dts laws/nns studied/vbn and/cc revised/vbn to/in the/at end/nn of/in modernizing/vbg and/cc improving/vbg them/ppo ''/'' ./.\n",
      "\n",
      "\n",
      "\tThe/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt it/pps said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb generally/rb accepted/vbn practices/nns which/wdt inure/vb to/in the/at best/jjt interest/nn of/in both/abx governments/nns ''/'' ./.\n",
      "\n",
      "\n",
      "\n",
      "Merger/nn-hl proposed/vbn-hl \n",
      "However/wrb ,/, the/at jury/nn said/vbd it/pps believes/vbz ``/`` these/dts two/cd offices/nns should/md be/be combined/vbn to/to achieve/vb greater/jjr efficiency/nn and/cc reduce/vb the/at cost/nn of/in administration/nn ''/'' ./.\n",
      "\n",
      "\n",
      "\tThe/at City/nn-tl Purchasing/vbg-tl Department/nn-tl ,/, the/at jury/nn said/vbd ,/, ``/`` is/bez lacking/vbg in/in experienced/vbn clerical/jj personnel/nns as/cs a/at result/nn of/in city/nn personnel/nns policies/nns ''/'' ./.\n",
      "It/pps urged/vbd that/cs the/at city/nn ``/`` take/vb steps/nns to/to remedy/vb ''/'' this/dt problem/nn ./.\n",
      "\n",
      "\n",
      "\tImplementation/nn of/in Georgia's/np$ automobile/nn title/nn law/nn was/bedz also/rb recommended/vbn by/in the/at outgoing/jj jury/nn ./.\n",
      "\n",
      "\n",
      "\tIt/pps urged/vbd that/cs the/at next/ap Legislature/nn-tl ``/`` provide/vb enabling/vbg funds/nns and/cc re-set/vb the/at effective/jj date/nn so/cs that/cs an/at orderly/jj implementation/nn of/in the/at law/nn may/md be/be effected/vbn ''/'' ./.\n",
      "\n",
      "\n",
      "\tThe/at grand/jj jury/nn took/vbd a/at swipe/nn at/in the/at State/nn-tl Welfare/nn-tl Department's/nn$-tl handling/nn of/in federal/jj funds/nns granted/vbn for/in child/nn welfare/nn services/nns in/in foster/jj homes/nns ./.\n",
      "\n",
      "\n",
      "\t``/`` This/dt is/bez one/cd of/in the/at major/jj items/nns in/in the/at Fulton/np-tl County/nn-tl general/jj assistance/nn program/nn ''/'' ,/, the/at jury/nn said/vbd ,/, but/cc the/at State/nn-tl Welfare/nn-tl Department/nn-tl ``/`` has/hvz seen/vbn fit/jj to/to distribute/vb these/dts funds/nns through/in the/at welfare/nn departments/nns of/in all/abn the/at counties/nns in/in the/at state/nn with/in the/at exception/nn of/in Fulton/np-tl County/nn-tl ,/, which/wdt receives/vbz none/pn of/in this/dt money/nn ./.\n",
      "\n",
      "\n",
      "\tThe/at jurors/nns said/vbd they/ppss realize/vb ``/`` a/at proportionate/jj distribution/nn of\n",
      "--------------------------------------------------\n",
      "Segment 1:\n",
      "\n",
      "\n",
      "\tThe/at Fulton/np-tl County/nn-tl Grand/jj-tl Jury/nn-tl said/vbd Friday/nr an/at investigation/nn of/in Atlanta's/np$ recent/jj primary/nn election/nn produced/vbd ``/`` no/at evidence/nn ''/'' that/cs any/dti irregularities/nns took/vbd place/nn ./.\n",
      "\n",
      "\n",
      "\tThe/at jury/nn further/rbr said/vbd in/in term-end/nn presentments/nns that/cs the/at City/nn-tl Executive/jj-tl Committee/nn-tl ,/, which/wdt had/hvd over-all/jj charge/nn of/in the/at election/nn ,/, ``/`` deserves/vbz the/at praise/nn and/cc thanks/nns of/in the/at City/nn-tl of/in-tl Atlanta/np-tl ''/'' for/in the/at manner/nn in/in which/wdt the/at election/nn was/bedz conducted/vbn ./.\n",
      "\n",
      "\n",
      "\tThe/at September-October/np term/nn jury/nn had/hvd been/ben charged/vbn by/in Fulton/np-tl Superior/jj-tl Court/nn-tl Judge/nn-tl Durwood/np Pye/np to/to investigate/vb reports/nns of/in possible/jj ``/`` irregularities/nns ''/'' in/in the/at hard-fought/jj primary/nn which/wdt was/bedz won/vbn by/in Mayor-nominate/nn-tl Ivan/np Allen/np Jr./np ./.\n",
      "--------------------------------------------------\n",
      "Segment 2:\n",
      "\n",
      "\n",
      "\n",
      "\t``/`` Only/rb a/at relative/jj handful/nn of/in such/jj reports/nns was/bedz received/vbn ''/'' ,/, the/at jury/nn said/vbd ,/, ``/`` considering/in the/at widespread/jj interest/nn in/in the/at election/nn ,/, the/at number/nn of/in voters/nns and/cc the/at size/nn of/in this/dt city/nn ''/'' ./.\n",
      "\n",
      "\n",
      "\tThe/at jury/nn said/vbd it/pps did/dod find/vb that/cs many/ap of/in Georgia's/np$ registration/nn and/cc election/nn laws/nns ``/`` are/ber outmoded/jj or/cc inadequate/jj and/cc often/rb ambiguous/jj ''/'' ./.\n",
      "\n",
      "\n",
      "\tIt/pps recommended/vbd that/cs Fulton/np legislators/nns act/vb ``/`` to/to have/hv these/dts laws/nns studied/vbn and/cc revised/vbn to/in the/at end/nn of/in modernizing/vbg and/cc improving/vbg them/ppo ''/'' ./.\n",
      "\n",
      "\n",
      "\tThe/at grand/jj jury/nn commented/vbd on/in a/at number/nn of/in other/ap topics/nns ,/, among/in them/ppo the/at Atlanta/np and/cc Fulton/np-tl County/nn-tl purchasing/vbg departments/nns which/wdt it/pps said/vbd ``/`` are/ber well/ql operated/vbn and/cc follow/vb generally/rb accepted/vbn practices/nns which/wdt inure/vb to/in the/at best/jjt interest/nn of/in both/abx governments/nns ''/'' ./.\n",
      "--------------------------------------------------\n",
      "Segment 3:\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Merger/nn-hl proposed/vbn-hl \n",
      "However/wrb ,/, the/at jury/nn said/vbd it/pps believes/vbz ``/`` these/dts two/cd offices/nns should/md be/be combined/vbn to/to achieve/vb greater/jjr efficiency/nn and/cc reduce/vb the/at cost/nn of/in administration/nn ''/'' ./.\n",
      "\n",
      "\n",
      "\tThe/at City/nn-tl Purchasing/vbg-tl Department/nn-tl ,/, the/at jury/nn said/vbd ,/, ``/`` is/bez lacking/vbg in/in experienced/vbn clerical/jj personnel/nns as/cs a/at result/nn of/in city/nn personnel/nns policies/nns ''/'' ./.\n",
      "It/pps urged/vbd that/cs the/at city/nn ``/`` take/vb steps/nns to/to remedy/vb ''/'' this/dt problem/nn ./.\n",
      "\n",
      "\n",
      "\tImplementation/nn of/in Georgia's/np$ automobile/nn title/nn law/nn was/bedz also/rb recommended/vbn by/in the/at outgoing/jj jury/nn ./.\n",
      "--------------------------------------------------\n",
      "Segment 4:\n",
      "\n",
      "\n",
      "\n",
      "\tIt/pps urged/vbd that/cs the/at next/ap Legislature/nn-tl ``/`` provide/vb enabling/vbg funds/nns and/cc re-set/vb the/at effective/jj date/nn so/cs that/cs an/at orderly/jj implementation/nn of/in the/at law/nn may/md be/be effected/vbn ''/'' ./.\n",
      "\n",
      "\n",
      "\tThe/at grand/jj jury/nn took/vbd a/at swipe/nn at/in the/at State/nn-tl Welfare/nn-tl Department's/nn$-tl handling/nn of/in federal/jj funds/nns granted/vbn for/in child/nn welfare/nn services/nns in/in foster/jj homes/nns ./.\n",
      "\n",
      "\n",
      "\t``/`` This/dt is/bez one/cd of/in the/at major/jj items/nns in/in the/at Fulton/np-tl County/nn-tl general/jj assistance/nn program/nn ''/'' ,/, the/at jury/nn said/vbd ,/, but/cc the/at State/nn-tl Welfare/nn-tl Department/nn-tl ``/`` has/hvz seen/vbn fit/jj to/to distribute/vb these/dts funds/nns through/in the/at welfare/nn departments/nns of/in all/abn the/at counties/nns in/in the/at state/nn with/in the/at exception/nn of/in Fulton/np-tl County/nn-tl ,/, which/wdt receives/vbz none/pn of/in this/dt money/nn ./.\n",
      "\n",
      "\n",
      "\tThe/at jurors/nns said/vbd they/ppss realize/vb ``/`` a/at proportionate/jj distribution/nn of\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries and modules\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "\n",
    "# Download the required data (only need to do this once)\n",
    "nltk.download('brown')\n",
    "\n",
    "# Load the Brown corpus text\n",
    "text = brown.raw()[:4000]  # Take a small portion of the text for demonstration\n",
    "\n",
    "# Initialize the TextTilingTokenizer with default parameters\n",
    "tt = TextTilingTokenizer()\n",
    "\n",
    "# Tokenize the text into topical sections\n",
    "segmented_text = tt.tokenize(text)\n",
    "\n",
    "# Print the original text\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Print the segmented text\n",
    "for i, segment in enumerate(segmented_text):\n",
    "    print(f\"Segment {i + 1}:\")\n",
    "    print(segment)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15114d23",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No paragraph breaks were found(text too short perhaps?)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\texttiling.py:244\u001b[0m, in \u001b[0;36mTextTilingTokenizer._create_token_table\u001b[1;34m(self, token_sequences, par_breaks)\u001b[0m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 244\u001b[0m     current_par_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(pb_iter)  \u001b[38;5;66;03m# skip break at 0\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mStopIteration\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m tt \u001b[38;5;241m=\u001b[39m TextTilingTokenizer()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Tokenize the text into topical sections\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m segmented_text \u001b[38;5;241m=\u001b[39m \u001b[43mtt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Print the original text\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\texttiling.py:115\u001b[0m, in \u001b[0;36mTextTilingTokenizer.tokenize\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m tokseqs:\n\u001b[0;32m    111\u001b[0m     ts\u001b[38;5;241m.\u001b[39mwrdindex_list \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    112\u001b[0m         wi \u001b[38;5;28;01mfor\u001b[39;00m wi \u001b[38;5;129;01min\u001b[39;00m ts\u001b[38;5;241m.\u001b[39mwrdindex_list \u001b[38;5;28;01mif\u001b[39;00m wi[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstopwords\n\u001b[0;32m    113\u001b[0m     ]\n\u001b[1;32m--> 115\u001b[0m token_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_token_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokseqs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnopunct_par_breaks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;66;03m# End of the Tokenization step\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \n\u001b[0;32m    118\u001b[0m \u001b[38;5;66;03m# Lexical score determination\u001b[39;00m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_method \u001b[38;5;241m==\u001b[39m BLOCK_COMPARISON:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\nltk\\tokenize\\texttiling.py:246\u001b[0m, in \u001b[0;36mTextTilingTokenizer._create_token_table\u001b[1;34m(self, token_sequences, par_breaks)\u001b[0m\n\u001b[0;32m    244\u001b[0m         current_par_break \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(pb_iter)  \u001b[38;5;66;03m# skip break at 0\u001b[39;00m\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 246\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo paragraph breaks were found(text too short perhaps?)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ts \u001b[38;5;129;01min\u001b[39;00m token_sequences:\n\u001b[0;32m    250\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word, index \u001b[38;5;129;01min\u001b[39;00m ts\u001b[38;5;241m.\u001b[39mwrdindex_list:\n",
      "\u001b[1;31mValueError\u001b[0m: No paragraph breaks were found(text too short perhaps?)"
     ]
    }
   ],
   "source": [
    "example_text = \"This is an example text. It consists of multiple sentences. The Text Tiling method is used to segment it into coherent sections. Each section represents a different topic. The boundaries are identified based on shifts in vocabulary and lexical scores.\"\n",
    "# Initialize the TextTilingTokenizer with default parameters\n",
    "tt = TextTilingTokenizer()\n",
    "\n",
    "# Tokenize the text into topical sections\n",
    "segmented_text = tt.tokenize(example_text)\n",
    "\n",
    "# Print the original text\n",
    "print(\"Original Text:\")\n",
    "print(text)\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Print the segmented text\n",
    "for i, segment in enumerate(segmented_text):\n",
    "    print(f\"Segment {i + 1}:\")\n",
    "    print(segment)\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399f93d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
