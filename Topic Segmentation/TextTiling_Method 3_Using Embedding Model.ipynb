{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c81c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eabfb00",
   "metadata": {},
   "source": [
    "# Method 3: Using Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a904e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "choi_folder_path = \"data/choi\"\n",
    "\n",
    "# Function to extract segments from a file\n",
    "def extract_segments(file_path):\n",
    "    with open(file_path, \"r\") as file:\n",
    "        segmented_text = file.read()\n",
    "\n",
    "    # Split the text by \"==========\" \n",
    "    segments = segmented_text.strip().split(\"==========\")\n",
    "\n",
    "    # Remove any  whitespace from each segment and remove empty segments\n",
    "    segments = [segment.strip() for segment in segments if segment.strip()]\n",
    "    \n",
    "    return segments\n",
    "\n",
    "data = []\n",
    "\n",
    "# Walk through all subdirectories of choi folder\n",
    "for root, _, files in os.walk(choi_folder_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".ref\"):\n",
    "            file_path = os.path.join(root, file)\n",
    "            segments = extract_segments(file_path)\n",
    "            united_text = \" \".join(segments)  # Combine segments into a single text\n",
    "            data.append({\n",
    "                \"File\": file_path,\n",
    "                \"Number of segments\": len(segments),\n",
    "                \"segments\": segments,\n",
    "                \"united_text\": united_text\n",
    "            })\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# df.to_csv(\"segments_data_with_united_text.csv\", index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d257de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remPunct = True):\n",
    "    # Preprocess the text by converting to lowercase and removing punctuation\n",
    "    text = text.lower()\n",
    "    if remPunct :\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def sentence_embed(sentences, model_name='paraphrase-MiniLM-L6-v2'):\n",
    "    # Load the SentenceTransformer model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Get sentence embeddings using the model\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    return embeddings\n",
    "def calculate_sentence_similarity(embeddings):\n",
    "    # Calculate cosine similarity between adjacent sentence embeddings\n",
    "    similarities = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        similarity = np.dot(embeddings[i], embeddings[i + 1]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "def topic_segmentation_embed(text, threshold=0.5):\n",
    "    # Perform text segmentation based on sentence embeddings\n",
    "\n",
    "    text = preprocess_text(text, remPunct = False)\n",
    "\n",
    "    # Step 1: Tokenize sentences\n",
    "    sentences = text.split('.')\n",
    "\n",
    "    # Step 2: Vectorize sentences using SentenceTransformer\n",
    "    embeddings = sentence_embed(sentences)\n",
    "\n",
    "    # Step 3: Calculate sentence similarity\n",
    "    sentence_similarity = calculate_sentence_similarity(embeddings)\n",
    "\n",
    "    # Step 4: Identify potential boundaries based on sentence similarity\n",
    "    boundaries = [0]\n",
    "\n",
    "    for i in range(1, len(sentence_similarity)):\n",
    "        if sentence_similarity[i] < threshold:\n",
    "            boundaries.append(i)\n",
    "\n",
    "    boundaries.append(len(sentence_similarity))\n",
    "\n",
    "    # Step 5: Segment the text into topics based on the identified boundaries\n",
    "    topics = []\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        start = boundaries[i]\n",
    "        end = boundaries[i + 1]\n",
    "        topic_text = '.'.join(sentences[start:end])\n",
    "        topics.append(topic_text)\n",
    "\n",
    "    return topics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57d488f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Text segmentation, also known as text splitting, is the process of dividing a continuous text into segments or sections based on some patterns or criteria. These segments are intended to represent different topics or themes present in the text. Text segmentation is a common technique used in natural language processing (NLP) and information retrieval tasks.\n",
    "\n",
    "There are several methods and algorithms for text segmentation. One such method is called TextTiling. TextTiling is a technique developed by Marti Hearst in 1994. It is mainly used for segmenting longer texts, such as essays, articles, or documents. TextTiling relies on finding patterns in word frequencies and co-occurrences to identify boundaries between different topics.\n",
    "\n",
    "In this example, we will implement a basic Python code to perform topic segmentation using SentenceTransformer to vectorize sentences. SentenceTransformer provides pre-trained models for sentence embeddings, which allow us to capture semantic meaning and similarity between sentences.\n",
    "\n",
    "Let's get started with the implementation.\n",
    "\"\"\"\n",
    "\n",
    "segmented_topics = topic_segmentation_embed(sample_text, threshold=0.4)\n",
    "for i, topic in enumerate(segmented_topics, start=1):\n",
    "    print(f\"Topic {i}:\")\n",
    "    print(topic.strip())\n",
    "    print(\"-----------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8d23d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = []\n",
    "num_topics = []\n",
    "\n",
    "for i in range(20) :\n",
    "    thres = 0 + (i+1)/20.0\n",
    "    segmented_topics = topic_segmentation_embed(sample_text, threshold=thres)\n",
    "    threshold.append(thres)\n",
    "    num_topics.append(len(segmented_topics))\n",
    "    \n",
    "plt.plot(threshold, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b3248d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = df['united_text'][0]\n",
    "segmented_topics = topic_segmentation_embed(sample_text, threshold=0.4)\n",
    "for i, topic in enumerate(segmented_topics, start=1):\n",
    "    print(f\"Topic {i}:\")\n",
    "    print(topic.strip())\n",
    "    print(\"-----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cbd93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = []\n",
    "num_topics = []\n",
    "\n",
    "for i in range(20) :\n",
    "    thres = 0 + (i+1)/20.0\n",
    "    segmented_topics = topic_segmentation_embed(sample_text, threshold=thres)\n",
    "    threshold.append(thres)\n",
    "    num_topics.append(len(segmented_topics))\n",
    "    \n",
    "plt.plot(threshold, num_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f3e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remPunct = True):\n",
    "    # Preprocess the text by converting to lowercase and removing punctuation\n",
    "    text = text.lower()\n",
    "    if remPunct :\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def sentence_embed(sentences, model_name='paraphrase-MiniLM-L6-v2'):\n",
    "    # Load the SentenceTransformer model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Get sentence embeddings using the model\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    return embeddings\n",
    "def calculate_sentence_similarity(embeddings):\n",
    "    # Calculate cosine similarity between adjacent sentence embeddings\n",
    "    similarities = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        similarity = np.dot(embeddings[i], embeddings[i + 1]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "def calculate_cosine_similarity(sentence1, sentence2):\n",
    "    # Calculate cosine similarity between two sentences\n",
    "\n",
    "    def vectorize(sentence):\n",
    "        words = sentence.split()\n",
    "        word_freq = {word: words.count(word) for word in set(words)}\n",
    "        return word_freq\n",
    "\n",
    "    vec_sentence1 = vectorize(sentence1)\n",
    "    vec_sentence2 = vectorize(sentence2)\n",
    "\n",
    "    intersection = set(vec_sentence1.keys()) & set(vec_sentence2.keys())\n",
    "    dot_product = sum(vec_sentence1[word] * vec_sentence2[word] for word in intersection)\n",
    "\n",
    "    magnitude1 = math.sqrt(sum(vec_sentence1[word] ** 2 for word in vec_sentence1))\n",
    "    magnitude2 = math.sqrt(sum(vec_sentence2[word] ** 2 for word in vec_sentence2))\n",
    "    \n",
    "    if magnitude1 * magnitude2 == 0 :\n",
    "        return 0\n",
    "    else :\n",
    "        return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "def topic_segmentation_embed(text,topic_segmentation_embed, num_topics=10):\n",
    "    # Perform text segmentation based on sentence embeddings\n",
    "\n",
    "    text = preprocess_text(text, remPunct=False)\n",
    "\n",
    "    # Step 1: Tokenize sentences\n",
    "    sentences = text.split('.')\n",
    "\n",
    "    # Step 2: Vectorize sentences using SentenceTransformer\n",
    "    embeddings = sentence_embed(sentences)\n",
    "\n",
    "    # Step 3: Calculate sentence similarity\n",
    "    sentence_similarity = calculate_sentence_similarity(embeddings)\n",
    "\n",
    "    # Calculate the number of sentences per topic\n",
    "    sentences_per_topic = len(sentences) // num_topics\n",
    "\n",
    "    # Step 4: Identify boundaries by selecting sentences that maximize dissimilarity\n",
    "    boundaries = [0]\n",
    "\n",
    "    for _ in range(num_topics - 1):\n",
    "        start = boundaries[-1]\n",
    "        end_candidates = range(start + sentences_per_topic, len(sentence_similarity))\n",
    "        max_dissimilarity_index = max(end_candidates, key=lambda i: max(sentence_similarity[start:i]))\n",
    "        \n",
    "        # Check if the maximum similarity in the selected range is below the threshold\n",
    "        if max(sentence_similarity[start:max_dissimilarity_index]) < threshold:\n",
    "            boundaries.append(max_dissimilarity_index)\n",
    "        else:\n",
    "            # If the threshold is not met, adjust the range to include more sentences\n",
    "            for i in end_candidates:\n",
    "                if max(sentence_similarity[start:i]) >= threshold:\n",
    "                    max_dissimilarity_index = i - 1\n",
    "                    boundaries.append(max_dissimilarity_index)\n",
    "                    break\n",
    "\n",
    "    boundaries.append(len(sentence_similarity))\n",
    "\n",
    "    # Step 5: Segment the text into topics based on the identified boundaries\n",
    "    topics = []\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        start = boundaries[i]\n",
    "        end = boundaries[i + 1]\n",
    "        topic_text = '.'.join(sentences[start:end])\n",
    "        topics.append(topic_text)\n",
    "\n",
    "    return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9e8209",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = df['united_text'][0]\n",
    "segmented_topics = topic_segmentation_embed(sample_text)\n",
    "for i, topic in enumerate(segmented_topics, start=1):\n",
    "    print(f\"Topic {i}:\")\n",
    "    print(topic.strip())\n",
    "    print(\"-----------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8107c31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_segments = df['segments'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b665e4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def preprocess_text(text, remPunct=True):\n",
    "    # Preprocess the text by converting to lowercase and removing punctuation\n",
    "    text = text.lower()\n",
    "    if remPunct:\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "def sentence_embed(sentences, model_name='paraphrase-MiniLM-L6-v2'):\n",
    "    # Load the SentenceTransformer model\n",
    "    model = SentenceTransformer(model_name)\n",
    "\n",
    "    # Get sentence embeddings using the model\n",
    "    embeddings = model.encode(sentences)\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "def calculate_sentence_similarity(embeddings):\n",
    "    # Calculate cosine similarity between adjacent sentence embeddings\n",
    "    similarities = []\n",
    "    for i in range(len(embeddings) - 1):\n",
    "        similarity = np.dot(embeddings[i], embeddings[i + 1]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[i + 1]))\n",
    "        similarities.append(similarity)\n",
    "    \n",
    "    return similarities\n",
    "\n",
    "def calculate_cosine_similarity(sentence1, sentence2):\n",
    "    # Calculate cosine similarity between two sentences\n",
    "\n",
    "    def vectorize(sentence):\n",
    "        words = sentence.split()\n",
    "        word_freq = {word: words.count(word) for word in set(words)}\n",
    "        return word_freq\n",
    "\n",
    "    vec_sentence1 = vectorize(sentence1)\n",
    "    vec_sentence2 = vectorize(sentence2)\n",
    "\n",
    "    intersection = set(vec_sentence1.keys()) & set(vec_sentence2.keys())\n",
    "    dot_product = sum(vec_sentence1[word] * vec_sentence2[word] for word in intersection)\n",
    "\n",
    "    magnitude1 = math.sqrt(sum(vec_sentence1[word] ** 2 for word in vec_sentence1))\n",
    "    magnitude2 = math.sqrt(sum(vec_sentence2[word] ** 2 for word in vec_sentence2))\n",
    "    \n",
    "    if magnitude1 * magnitude2 == 0 :\n",
    "        return 0\n",
    "    else :\n",
    "        return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "def topic_segmentation_embed(text, threshold=0.5, num_topics=10):\n",
    "    # Perform text segmentation based on sentence embeddings\n",
    "\n",
    "    text = preprocess_text(text, remPunct=False)\n",
    "\n",
    "    # Step 1: Tokenize sentences\n",
    "    sentences = text.split('.')\n",
    "\n",
    "    # Step 2: Vectorize sentences using SentenceTransformer\n",
    "    embeddings = sentence_embed(sentences)\n",
    "\n",
    "    # Step 3: Calculate sentence similarity\n",
    "    sentence_similarity = calculate_sentence_similarity(embeddings)\n",
    "\n",
    "    # Calculate the number of sentences per topic\n",
    "    sentences_per_topic = len(sentences) // num_topics\n",
    "\n",
    "    # Step 4: Identify boundaries by selecting sentences that maximize dissimilarity\n",
    "    boundaries = [0]\n",
    "\n",
    "    for _ in range(num_topics - 1):\n",
    "        start = boundaries[-1]\n",
    "        end_candidates = range(start + sentences_per_topic, len(sentence_similarity))\n",
    "        max_dissimilarity_index = None\n",
    "\n",
    "        for i in end_candidates:\n",
    "            if max(sentence_similarity[start:i]) >= threshold:\n",
    "                max_dissimilarity_index = i - 1\n",
    "                break\n",
    "\n",
    "        if max_dissimilarity_index is None:\n",
    "            # If no suitable index was found, break the loop\n",
    "            break\n",
    "\n",
    "        boundaries.append(max_dissimilarity_index)\n",
    "\n",
    "    boundaries.append(len(sentence_similarity))\n",
    "\n",
    "    # Step 5: Segment the text into topics based on the identified boundaries\n",
    "    topics = []\n",
    "    for i in range(len(boundaries) - 1):\n",
    "        start = boundaries[i]\n",
    "        end = boundaries[i + 1]\n",
    "        topic_text = '.'.join(sentences[start:end])\n",
    "        topics.append(topic_text)\n",
    "\n",
    "    return topics\n",
    "\n",
    "# List of threshold values to test\n",
    "threshold_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Calculate similarity scores for each threshold\n",
    "for threshold in threshold_range:\n",
    "    predicted_segments = topic_segmentation_embed(sample_text, threshold=threshold)\n",
    "    \n",
    "    # Check if the number of predicted segments is 10\n",
    "    if len(predicted_segments) == 10:\n",
    "        # Calculate similarity score for the obtained segments and actual segments\n",
    "        similarity_scores = []\n",
    "        for obtained_segment in predicted_segments:\n",
    "            max_similarity = 0\n",
    "            for actual_segment in actual_segments:\n",
    "                similarity = calculate_cosine_similarity(obtained_segment, actual_segment)\n",
    "                max_similarity = max(max_similarity, similarity)\n",
    "            similarity_scores.append(max_similarity)\n",
    "\n",
    "        # Print or store the similarity scores for this threshold\n",
    "        print(f\"Threshold: {threshold}, Similarity Scores: {similarity_scores}\")\n",
    "    else:\n",
    "        # If not 10 segments, add zero similarity scores\n",
    "        similarity_scores = [0] * len(predicted_segments)\n",
    "        print(f\"Threshold: {threshold}, Number of Predicted Segments: {len(predicted_segments)}, Similarity Scores: {similarity_scores}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed947a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of threshold values to test\n",
    "threshold_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Calculate and print the average similarity scores for each threshold\n",
    "for threshold in threshold_range:\n",
    "    predicted_segments = topic_segmentation_embed(sample_text, threshold=threshold)\n",
    "    \n",
    "    # Check if the number of predicted segments is 10\n",
    "    if len(predicted_segments) == 10:\n",
    "        # Calculate similarity score for the obtained segments and actual segments\n",
    "        similarity_scores = []\n",
    "        for obtained_segment in predicted_segments:\n",
    "            max_similarity = 0\n",
    "            for actual_segment in actual_segments:\n",
    "                similarity = calculate_cosine_similarity(obtained_segment, actual_segment)\n",
    "                max_similarity = max(max_similarity, similarity)\n",
    "            similarity_scores.append(max_similarity)\n",
    "\n",
    "        # Calculate the average similarity score\n",
    "        avg_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "        \n",
    "        # Print the average similarity score for this threshold\n",
    "        print(f\"Threshold: {threshold}, Average Similarity Score: {avg_similarity}\")\n",
    "    else:\n",
    "        # If not 10 segments, print an average similarity score of 0\n",
    "        print(f\"Threshold: {threshold}, Number of Predicted Segments: {len(predicted_segments)}, Average Similarity Score: 0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f478e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# List of threshold values to test\n",
    "threshold_range = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "\n",
    "# Calculate the average similarity score for a given threshold\n",
    "def calculate_average_similarity(predicted_segments, actual_segments):\n",
    "    if len(predicted_segments) != 10:\n",
    "        return 0\n",
    "    \n",
    "    similarity_scores = []\n",
    "    for obtained_segment in predicted_segments:\n",
    "        max_similarity = 0\n",
    "        for actual_segment in actual_segments:\n",
    "            similarity = calculate_cosine_similarity(obtained_segment, actual_segment)\n",
    "            max_similarity = max(max_similarity, similarity)\n",
    "        similarity_scores.append(max_similarity)\n",
    "    \n",
    "    avg_similarity = sum(similarity_scores) / len(similarity_scores)\n",
    "    return avg_similarity\n",
    "\n",
    "# Calculate and store the average similarity scores for each threshold\n",
    "for threshold in threshold_range:\n",
    "    df[f'Predicted_segments_{threshold}'] = df['united_text'].apply(lambda text: topic_segmentation_embed(text, threshold=threshold))\n",
    "    df[f'Avg_score_{threshold}'] = df.apply(lambda row: calculate_average_similarity(row[f'Predicted_segments_{threshold}'], row['segments']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d34a1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6cd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of columns for which you want to calculate the average\n",
    "columns_to_average = ['Avg_score_0.1', 'Avg_score_0.2', 'Avg_score_0.3','Avg_score_0.4','Avg_score_0.5','Avg_score_0.6','Avg_score_0.7',\n",
    "                      'Avg_score_0.8','Avg_score_0.9','Avg_score_1.0']\n",
    "\n",
    "# Calculate the average values for the specified columns\n",
    "average_values = df[columns_to_average].mean()\n",
    "\n",
    "# Print the average values\n",
    "print(average_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b6bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
