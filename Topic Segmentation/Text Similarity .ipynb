{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bafd2f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os               \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071939d6",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/text-tiling-done-right-building-solid-foundations-for-your-personal-llm-e70947779ac1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1df36",
   "metadata": {},
   "source": [
    "https://medium.com/msackiit/what-is-text-similarity-and-how-to-implement-it-c74c8b641883"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "313df69b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-web-design-trends-.json\n",
      "locate-empty-directo.json\n",
      "overview-of-spiral-s.json\n",
      "13-code-quality-metr.json\n",
      "can-value-stream-man.json\n",
      "challenges-and-check.json\n",
      "competition-of-the-m.json\n",
      "connect-memphis-as-a.json\n",
      "correlations-made-ea.json\n",
      "creating-crap-faster.json\n",
      "Files not read in:  0\n",
      "Files read in:  1050\n",
      "                                     header_title  \\\n",
      "0   Convert Fahrenheit to Celsius with JavaScript   \n",
      "1     Create a Thumbnail From a Video with ffmpeg   \n",
      "2                      CSS ::file-selector-button   \n",
      "3                Customizing HTML Form Validation   \n",
      "4  Detect Browser Bars Visibility with JavaScript   \n",
      "\n",
      "                              date  \\\n",
      "0  Wed, 26 Oct 2022 10:19:49 +0000   \n",
      "1  Tue, 25 Oct 2022 09:28:58 +0000   \n",
      "2  Mon, 20 Feb 2023 09:50:57 +0000   \n",
      "3  Mon, 09 Jan 2023 10:57:00 +0000   \n",
      "4  Fri, 30 Dec 2022 01:36:35 +0000   \n",
      "\n",
      "                                                text  \\\n",
      "0  The United States is one of the last bodies th...   \n",
      "1  Creating a thumbnail to represent a video is a...   \n",
      "2  We all love beautifully styled form controls b...   \n",
      "3  Form validation has always been my least favor...   \n",
      "4  It's one thing to know about what's in the bro...   \n",
      "\n",
      "                                             summary  \\\n",
      "0  The article discusses the importance of provid...   \n",
      "1  The article discusses several topics related t...   \n",
      "2  This article discusses various web development...   \n",
      "3  The article discusses how to control native fo...   \n",
      "4  The article discusses how developers can gain ...   \n",
      "\n",
      "                                                 url  \n",
      "0  https://davidwalsh.name/fahrenheit-celsius-jav...  \n",
      "1    https://davidwalsh.name/create-thumbnail-ffmpeg  \n",
      "2   https://davidwalsh.name/css-file-selector-button  \n",
      "3           https://davidwalsh.name/html5-validation  \n",
      "4        https://davidwalsh.name/detect-browser-bars  \n"
     ]
    }
   ],
   "source": [
    "# Relative path\n",
    "path_to_data = 'rssDevData/'\n",
    "\n",
    "# Names of 5 blog categories to import \n",
    "blogs = ['DavidWalsh','DeveloperDotCom','DZone','GeeksForGeeks','SCAND','SDTimes']\n",
    "\n",
    "# Identify key to blog text in JSON file\n",
    "key = 'text'\n",
    "\n",
    "# Initialize empty data frame\n",
    "text_data = pd.DataFrame()\n",
    "\n",
    "#Initialize counter to keep track of files that fail\n",
    "files_not_read = 0\n",
    "\n",
    "category_sizes = [] # To store number of blog articles by category\n",
    "labels = [] # To store \"true\" labels ->  [0, 1, 2, 3, 4]\n",
    "label = 0\n",
    "\n",
    "for blog in blogs :\n",
    "    current_blog_category_count = 0\n",
    "    \n",
    "    path_to_blogs = path_to_data + blog \n",
    "    \n",
    "    for root, dir, files in os.walk(path_to_blogs) :\n",
    "        # get list of only json files\n",
    "        json_files = [pos_json for pos_json in files if pos_json.endswith('.json')]\n",
    "\n",
    "        for j in json_files :\n",
    "            with open(root + '/' + j, 'r') as f:\n",
    "                try:\n",
    "                    data = json.load(f)\n",
    "                    if len(data[key]) > 200 :\n",
    "                        try : # fails when there is more than one json object in the file \n",
    "                            text_data = pd.concat([text_data, pd.DataFrame(data, index=[0])], ignore_index=True) \n",
    "                            current_blog_category_count += 1 \n",
    "                            labels.append(label) \n",
    "                        except : \n",
    "                            files_not_read += 1\n",
    "                except:\n",
    "                    print(j)\n",
    "print('Files not read in: ', str(files_not_read))\n",
    "print('Files read in: ', str(len(text_data)))\n",
    "print(text_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6077a75",
   "metadata": {},
   "source": [
    "https://spotintelligence.com/2022/12/19/text-similarity-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f0a9d6",
   "metadata": {},
   "source": [
    "## 1. Text similarity with NLTK (Lexical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "356fbe72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def sentence_similarity_NLTK(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize and lemmatize the sentences\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokenized_sentences.append(tokens)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_sentences = []\n",
    "    for tokens in tokenized_sentences:\n",
    "        filtered_sentence = [token for token in tokens if token not in stop_words]\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    # Create the TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectors = vectorizer.fit_transform([' '.join(sentence) for sentence in filtered_sentences])\n",
    "\n",
    "    # Calculate the cosine similarity for each pair of sentences\n",
    "    similarity_scores = cosine_similarity(tfidf_vectors)\n",
    "\n",
    "    # Store the similarity scores along with the sentence indices\n",
    "    similarity_results = []\n",
    "    num_sentences = len(sentences)\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(i + 1, num_sentences):\n",
    "            similarity_score = similarity_scores[i][j]\n",
    "            similarity_results.append((i, j, round(similarity_score,3)))\n",
    "\n",
    "    return similarity_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f7d6ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0.0),\n",
       " (0, 2, 0.152),\n",
       " (0, 3, 0.0),\n",
       " (0, 4, 0.0),\n",
       " (1, 2, 0.0),\n",
       " (1, 3, 0.0),\n",
       " (1, 4, 0.0),\n",
       " (2, 3, 0.114),\n",
       " (2, 4, 0.089),\n",
       " (3, 4, 0.0)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_text = \"This is an example text. It consists of multiple sentences. The Text Tiling method is used to segment it into coherent sections. Each section represents a different topic. The boundaries are identified based on shifts in vocabulary and lexical scores.\"\n",
    "sentence_similarity_NLTK(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2249d128",
   "metadata": {},
   "source": [
    "## NLTK Modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f99f879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def sentence_similarity_NLTK_m(text, max_df=1.0, min_df=1, ngram_range=(1, 1), use_idf=True, smooth_idf=True, sublinear_tf=False):\n",
    "    # Split the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "\n",
    "    # Tokenize and lemmatize the sentences\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokenized_sentences = []\n",
    "    for sentence in sentences:\n",
    "        tokens = word_tokenize(sentence)\n",
    "        tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "        tokenized_sentences.append(tokens)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    filtered_sentences = []\n",
    "    for tokens in tokenized_sentences:\n",
    "        filtered_sentence = [token for token in tokens if token not in stop_words]\n",
    "        filtered_sentences.append(filtered_sentence)\n",
    "\n",
    "    # Create the TF-IDF vectors with custom hyperparameters\n",
    "    vectorizer = TfidfVectorizer(max_df=max_df, min_df=min_df, ngram_range=ngram_range, use_idf=use_idf, smooth_idf=smooth_idf, sublinear_tf=sublinear_tf)\n",
    "    tfidf_vectors = vectorizer.fit_transform([' '.join(sentence) for sentence in filtered_sentences])\n",
    "\n",
    "    # Calculate the cosine similarity for each pair of sentences\n",
    "    similarity_scores = cosine_similarity(tfidf_vectors)\n",
    "\n",
    "    # Store the similarity scores along with the sentence indices\n",
    "    similarity_results = []\n",
    "    num_sentences = len(sentences)\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(i + 1, num_sentences):\n",
    "            similarity_score = similarity_scores[i][j]\n",
    "            similarity_results.append((i, j, round(similarity_score, 3)))\n",
    "\n",
    "    return similarity_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff8c75d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0.0),\n",
       " (0, 2, 0.152),\n",
       " (0, 3, 0.0),\n",
       " (0, 4, 0.0),\n",
       " (1, 2, 0.0),\n",
       " (1, 3, 0.0),\n",
       " (1, 4, 0.0),\n",
       " (2, 3, 0.114),\n",
       " (2, 4, 0.089),\n",
       " (3, 4, 0.0)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_similarity_NLTK_m(example_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90403ed4",
   "metadata": {},
   "source": [
    "## 2. Text similarity with Scikit-Learn (Lexical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dbf2d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import re\n",
    "\n",
    "def sentence_similarity_Scikit_Learn(text):\n",
    "    # Split the text into sentences\n",
    "    sentences = re.split(r'[.!?]', text)\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip() != \"\"]\n",
    "\n",
    "    # Create the TF-IDF vectors\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_vectors = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    # Calculate the cosine similarity for each pair of sentences\n",
    "    similarity_scores = cosine_similarity(tfidf_vectors)\n",
    "\n",
    "#     return similarity_scores\n",
    "\n",
    "    # Store the similarity scores along with the sentence indices\n",
    "    similarity_results = []\n",
    "    num_sentences = len(sentences)\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(i + 1, num_sentences):\n",
    "            similarity_score = similarity_scores[i][j]\n",
    "            similarity_results.append((i, j, round(similarity_score,3)))\n",
    "\n",
    "    return similarity_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b586e50e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0.0),\n",
       " (0, 2, 0.196),\n",
       " (0, 3, 0.0),\n",
       " (0, 4, 0.0),\n",
       " (1, 2, 0.0),\n",
       " (1, 3, 0.0),\n",
       " (1, 4, 0.0),\n",
       " (2, 3, 0.0),\n",
       " (2, 4, 0.0),\n",
       " (3, 4, 0.0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_similarity_Scikit_Learn(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ce44e443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_similarity_Scikit_Learn(text_data['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db620113",
   "metadata": {},
   "source": [
    "## 3.Text similarity with BERT (Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bc08bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def sentence_similarity_BERT(text):\n",
    "    # Load the BERT model and tokenizer\n",
    "    model = transformers.BertModel.from_pretrained('bert-base-uncased')\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('.')\n",
    "\n",
    "    # Remove empty sentences\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip() != \"\"]\n",
    "\n",
    "    # Encode each sentence to obtain embeddings\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            output = model(**tokens)  # Pass the tokens as a dictionary\n",
    "            encoding = output.last_hidden_state.mean(dim=1).squeeze(0)  # Calculate sentence embeddings using the mean pooling\n",
    "        embeddings.append(encoding)\n",
    "\n",
    "    # Calculate the cosine similarity for each pair of sentences\n",
    "    similarity_scores = np.zeros((len(embeddings), len(embeddings)))\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            similarity_scores[i][j] = np.dot(embeddings[i], embeddings[j]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))\n",
    "            similarity_scores[j][i] = similarity_scores[i][j]  # As cosine similarity is symmetric\n",
    "\n",
    "    #     return similarity_scores\n",
    "\n",
    "    # Store the similarity scores along with the sentence indices\n",
    "    similarity_results = []\n",
    "    num_sentences = len(sentences)\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(i + 1, num_sentences):\n",
    "            similarity_score = similarity_scores[i][j]\n",
    "            similarity_results.append((i, j, round(similarity_score,3)))\n",
    "\n",
    "    return similarity_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c529089",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0.735),\n",
       " (0, 2, 0.767),\n",
       " (0, 3, 0.792),\n",
       " (0, 4, 0.676),\n",
       " (1, 2, 0.845),\n",
       " (1, 3, 0.847),\n",
       " (1, 4, 0.818),\n",
       " (2, 3, 0.844),\n",
       " (2, 4, 0.838),\n",
       " (3, 4, 0.822)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_similarity_BERT(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c086538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_similarity_BERT(text_data['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3609f0",
   "metadata": {},
   "source": [
    "## 4. Text similarity with RoBERTa (Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "92e0ad2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def sentence_similarity_RoBERTa(text):\n",
    "    # Load the RoBERTa model and tokenizer\n",
    "    model = transformers.RobertaModel.from_pretrained('roberta-base')\n",
    "    tokenizer = transformers.RobertaTokenizer.from_pretrained('roberta-base')\n",
    "\n",
    "    # Split the text into sentences\n",
    "    sentences = text.split('.')\n",
    "\n",
    "    # Remove empty sentences\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip() != \"\"]\n",
    "\n",
    "    # Encode each sentence to obtain embeddings\n",
    "    embeddings = []\n",
    "    for sentence in sentences:\n",
    "        tokens = tokenizer.encode_plus(sentence, add_special_tokens=True, max_length=512, padding='max_length', truncation=True, return_tensors='pt')\n",
    "        with torch.no_grad():\n",
    "            output = model(**tokens)  # Pass the tokens as a dictionary\n",
    "            encoding = output.last_hidden_state.mean(dim=1).squeeze(0)  # Calculate sentence embeddings using the mean pooling\n",
    "        embeddings.append(encoding)\n",
    "\n",
    "    # Calculate the cosine similarity for each pair of sentences\n",
    "    similarity_scores = np.zeros((len(embeddings), len(embeddings)))\n",
    "    for i in range(len(embeddings)):\n",
    "        for j in range(i + 1, len(embeddings)):\n",
    "            similarity_scores[i][j] = np.dot(embeddings[i], embeddings[j]) / (np.linalg.norm(embeddings[i]) * np.linalg.norm(embeddings[j]))\n",
    "            similarity_scores[j][i] = similarity_scores[i][j]  # As cosine similarity is symmetric\n",
    "\n",
    "#     return similarity_scores\n",
    "\n",
    "    # Store the similarity scores along with the sentence indices\n",
    "    similarity_results = []\n",
    "    num_sentences = len(sentences)\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(i + 1, num_sentences):\n",
    "            similarity_score = similarity_scores[i][j]\n",
    "            similarity_results.append((i, j, round(similarity_score,3)))\n",
    "\n",
    "    return similarity_results\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "84d949f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0.969),\n",
       " (0, 2, 0.791),\n",
       " (0, 3, 0.886),\n",
       " (0, 4, 0.846),\n",
       " (1, 2, 0.811),\n",
       " (1, 3, 0.919),\n",
       " (1, 4, 0.882),\n",
       " (2, 3, 0.94),\n",
       " (2, 4, 0.95),\n",
       " (3, 4, 0.986)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_similarity_RoBERTa(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4ff9db1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_similarity_RoBERTa(text_data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5dfb348f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sentence_transformers import SentenceTransformer\n",
    "# model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# sentences = [\n",
    "#     'the person wear red T-shirt',\n",
    "#     'this person is walking',\n",
    "#     'the boy wear red T-shirt'\n",
    "#     ]\n",
    "# sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "# for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "#     print(\"Sentence:\", sentence)\n",
    "#     print(\"Embedding:\", embedding)\n",
    "#     print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a241939",
   "metadata": {},
   "source": [
    "## 5. Text similarity with Sentence Transformers (Semantic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d0ea5",
   "metadata": {},
   "source": [
    "https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb32650",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/65199011/is-there-a-way-to-check-similarity-between-two-full-sentences-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "865fdfd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\msalehi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0.668),\n",
       " (0, 2, 0.604),\n",
       " (0, 3, 0.627),\n",
       " (0, 4, 0.482),\n",
       " (1, 2, 0.662),\n",
       " (1, 3, 0.784),\n",
       " (1, 4, 0.649),\n",
       " (2, 3, 0.595),\n",
       " (2, 4, 0.635),\n",
       " (3, 4, 0.632)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download the nltk sentence tokenizer data\n",
    "nltk.download('punkt')\n",
    "\n",
    "def sentence_similarities_Transformers(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = nltk.sent_tokenize(text)\n",
    "\n",
    "    # Initialize the SentenceTransformer model\n",
    "    model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "    # Encode the sentences into embeddings\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "\n",
    "    # Calculate the cosine similarity for each pair of sentences\n",
    "    similarity_results = []\n",
    "    num_sentences = len(sentences)\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(i + 1, num_sentences):\n",
    "            similarity = cosine_similarity(sentence_embeddings[i].reshape(1, -1), sentence_embeddings[j].reshape(1, -1))\n",
    "            similarity_score = round(similarity[0][0], 3)\n",
    "            similarity_results.append((i, j, similarity_score))\n",
    "\n",
    "    return similarity_results\n",
    "\n",
    "sentence_similarities_Transformers(example_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f353f877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_similarities_Transformers(text_data['text'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d17a48f",
   "metadata": {},
   "source": [
    "## 6. Text similarity with TFHub Universal Sentence Encoder (Semantic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a7c51",
   "metadata": {},
   "source": [
    "https://tfhub.dev/google/universal-sentence-encoder/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67daa3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_hub as hub\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def sentence_similarities_USE(text):\n",
    "    # Load the Universal Sentence Encoder model\n",
    "    embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
    "\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = text.split('.')  # Split the text by periods to get sentences\n",
    "\n",
    "    # Remove empty sentences resulting from the split\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    # Get the sentence embeddings\n",
    "    embeddings = embed(sentences)\n",
    "\n",
    "    # Calculate the similarity for each pair of sentences\n",
    "    similarity_results = []\n",
    "    num_sentences = len(sentences)\n",
    "    for i in range(num_sentences):\n",
    "        for j in range(i + 1, num_sentences):\n",
    "            similarity = 1.0 - cosine(embeddings[i], embeddings[j])\n",
    "            similarity_results.append((i, j, round(similarity, 3)))\n",
    "\n",
    "    return similarity_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "86f4d331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 1, 0.341),\n",
       " (0, 2, 0.257),\n",
       " (0, 3, 0.233),\n",
       " (0, 4, 0.104),\n",
       " (1, 2, 0.177),\n",
       " (1, 3, 0.29),\n",
       " (1, 4, 0.177),\n",
       " (2, 3, 0.177),\n",
       " (2, 4, 0.222),\n",
       " (3, 4, -0.032)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "sentence_similarities_USE(example_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4bd2b3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_similarities_USE(text_data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0cf4719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
