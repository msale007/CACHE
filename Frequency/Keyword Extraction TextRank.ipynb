{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e53733b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\msalehi\\AppData\\Roaming\\Python\\Python39\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import spacy\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from spacy.lang.en.stop_words import STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80b524f",
   "metadata": {},
   "source": [
    "## Read rss data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d222477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Json files\n",
    "path_to_json = 'rssData'\n",
    "json_files = [pos_json for pos_json in os.listdir(path_to_json) if pos_json.endswith('.json')]\n",
    "#print(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47d60319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go through Json files \n",
    "base_dir = 'rssData'\n",
    "\n",
    "#Get all files in the directory\n",
    "\n",
    "data_list = []\n",
    "for file in os.listdir(base_dir):\n",
    "\n",
    "    #If file is a json, construct it's full path and open it, append all json data to list\n",
    "    if 'json' in file:\n",
    "        json_path = os.path.join(base_dir, file)\n",
    "        json_data = pd.read_json(json_path, lines=True)\n",
    "        data_list.append(json_data)\n",
    "\n",
    "#print(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "200c435f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d798d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    \n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a402fd51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([''], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_list[0].text.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad16334d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get text data and remove empty texts\n",
    "all_text=[]\n",
    "for i in range(len(data_list)):\n",
    "    if (data_list[i].text.values!=''):\n",
    "        text=list(data_list[i].text)\n",
    "#         print(type(text))\n",
    "        all_text.append(text)\n",
    "        #print(data_list[i].text)\n",
    "        #print(text)\n",
    "#print(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba66ef1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2095c581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 911 service as it existed until July 28, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DanielMiessler Created/Updated: July 25, 2022 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 911 service as it exists today.For the pas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DanielMiesslerMy first thought on the whole di...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DanielMiesslerWell, our congressional heroes f...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  The 911 service as it existed until July 28, 2...\n",
       "1  DanielMiessler Created/Updated: July 25, 2022 ...\n",
       "2  The 911 service as it exists today.For the pas...\n",
       "3  DanielMiesslerMy first thought on the whole di...\n",
       "4  DanielMiesslerWell, our congressional heroes f..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_df= pd.DataFrame(all_text, columns=['text'])\n",
    "all_text_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43aaef44",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4b04e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to clean data\n",
    "#lower case\n",
    "#remove stop words\n",
    "#lemmatization\n",
    "\n",
    "def cleanData(doc):\n",
    "    doc = doc.lower()\n",
    "    doc = nlp(doc)\n",
    "    tokens = [tokens.lower_ for tokens in doc]\n",
    "    tokens = [tokens for tokens in doc if (tokens.is_stop == False)]\n",
    "    tokens = [tokens for tokens in tokens if (tokens.is_punct == False)]\n",
    "    final_token = [token.lemma_ for token in tokens]\n",
    "    \n",
    "    return \" \".join(final_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "79b4931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_text_df['clean'] = all_text_df.apply(lambda row:cleanData (row['text']),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb574e74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The 911 service as it existed until July 28, 2...</td>\n",
       "      <td>911 service exist july 28 2022.911[.]re proxy ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DanielMiessler Created/Updated: July 25, 2022 ...</td>\n",
       "      <td>danielmiessler create update july 25 2022 read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The 911 service as it exists today.For the pas...</td>\n",
       "      <td>911 service exist today.for past seven year on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DanielMiesslerMy first thought on the whole di...</td>\n",
       "      <td>danielmiesslermy think discussion sure musk ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DanielMiesslerWell, our congressional heroes f...</td>\n",
       "      <td>danielmiesslerwell congressional hero finally ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  The 911 service as it existed until July 28, 2...   \n",
       "1  DanielMiessler Created/Updated: July 25, 2022 ...   \n",
       "2  The 911 service as it exists today.For the pas...   \n",
       "3  DanielMiesslerMy first thought on the whole di...   \n",
       "4  DanielMiesslerWell, our congressional heroes f...   \n",
       "\n",
       "                                               clean  \n",
       "0  911 service exist july 28 2022.911[.]re proxy ...  \n",
       "1  danielmiessler create update july 25 2022 read...  \n",
       "2  911 service exist today.for past seven year on...  \n",
       "3  danielmiesslermy think discussion sure musk ar...  \n",
       "4  danielmiesslerwell congressional hero finally ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d7fc09",
   "metadata": {},
   "source": [
    "## Using TextRank4zh package to find most frequent words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95baced6",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/textrank-for-keyword-extraction-by-python-c0bae21bcec0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "174a7672",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRank4Keyword():\n",
    "    \"\"\"Extract keywords from text\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.d = 0.85 # damping coefficient, usually is .85\n",
    "        self.min_diff = 1e-5 # convergence threshold\n",
    "        self.steps = 10 # iteration steps\n",
    "        self.node_weight = None # save keywords and its weight\n",
    "\n",
    "    \n",
    "    def set_stopwords(self, stopwords):  \n",
    "        \"\"\"Set stop words\"\"\"\n",
    "        for word in STOP_WORDS.union(set(stopwords)):\n",
    "            lexeme = nlp.vocab[word]\n",
    "            lexeme.is_stop = True\n",
    "    \n",
    "    def sentence_segment(self, doc, candidate_pos, lower):\n",
    "        \"\"\"Store those words only in cadidate_pos\"\"\"\n",
    "        sentences = []\n",
    "        for sent in doc.sents:\n",
    "            selected_words = []\n",
    "            for token in sent:\n",
    "                # Store words only with cadidate POS tag\n",
    "                if token.pos_ in candidate_pos and token.is_stop is False:\n",
    "                    if lower is True:\n",
    "                        selected_words.append(token.text.lower())\n",
    "                    else:\n",
    "                        selected_words.append(token.text)\n",
    "            sentences.append(selected_words)\n",
    "        return sentences\n",
    "        \n",
    "    def get_vocab(self, sentences):\n",
    "        \"\"\"Get all tokens\"\"\"\n",
    "        vocab = OrderedDict()\n",
    "        i = 0\n",
    "        for sentence in sentences:\n",
    "            for word in sentence:\n",
    "                if word not in vocab:\n",
    "                    vocab[word] = i\n",
    "                    i += 1\n",
    "        return vocab\n",
    "    \n",
    "    def get_token_pairs(self, window_size, sentences):\n",
    "        \"\"\"Build token_pairs from windows in sentences\"\"\"\n",
    "        token_pairs = list()\n",
    "        for sentence in sentences:\n",
    "            for i, word in enumerate(sentence):\n",
    "                for j in range(i+1, i+window_size):\n",
    "                    if j >= len(sentence):\n",
    "                        break\n",
    "                    pair = (word, sentence[j])\n",
    "                    if pair not in token_pairs:\n",
    "                        token_pairs.append(pair)\n",
    "        return token_pairs\n",
    "        \n",
    "    def symmetrize(self, a):\n",
    "        return a + a.T - np.diag(a.diagonal())\n",
    "    \n",
    "    def get_matrix(self, vocab, token_pairs):\n",
    "        \"\"\"Get normalized matrix\"\"\"\n",
    "        # Build matrix\n",
    "        vocab_size = len(vocab)\n",
    "        g = np.zeros((vocab_size, vocab_size), dtype='float')\n",
    "        for word1, word2 in token_pairs:\n",
    "            i, j = vocab[word1], vocab[word2]\n",
    "            g[i][j] = 1\n",
    "            \n",
    "        # Get Symmeric matrix\n",
    "        g = self.symmetrize(g)\n",
    "        \n",
    "        # Normalize matrix by column\n",
    "        norm = np.sum(g, axis=0)\n",
    "        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "        \n",
    "        return g_norm\n",
    "\n",
    "    \n",
    "    def get_keywords(self, number=10):\n",
    "        \"\"\"Print top number keywords\"\"\"\n",
    "        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n",
    "        for i, (key, value) in enumerate(node_weight.items()):\n",
    "            print(key + ' - ' + str(value))\n",
    "            if i > number:\n",
    "                break\n",
    "        \n",
    "        \n",
    "    def analyze(self, text, \n",
    "                candidate_pos=['NOUN', 'VERB','ADJ', 'PROPN'], \n",
    "                window_size=4, lower=False, stopwords=list()):\n",
    "        \"\"\"Main function to analyze text\"\"\"\n",
    "        \n",
    "        # Set stop words\n",
    "        self.set_stopwords(stopwords)\n",
    "        \n",
    "        # Pare text by spaCy\n",
    "        doc = nlp(text)\n",
    "        \n",
    "        # Filter sentences\n",
    "        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n",
    "        \n",
    "        # Build vocabulary\n",
    "        vocab = self.get_vocab(sentences)\n",
    "        \n",
    "        # Get token_pairs from windows\n",
    "        token_pairs = self.get_token_pairs(window_size, sentences)\n",
    "        \n",
    "        # Get normalized matrix\n",
    "        g = self.get_matrix(vocab, token_pairs)\n",
    "        \n",
    "        # Initionlization for weight(pagerank value)\n",
    "        pr = np.array([1] * len(vocab))\n",
    "        \n",
    "        # Iteration\n",
    "        previous_pr = 0\n",
    "        for epoch in range(self.steps):\n",
    "            pr = (1-self.d) + self.d * np.dot(g, pr)\n",
    "            if abs(previous_pr - sum(pr))  < self.min_diff:\n",
    "                break\n",
    "            else:\n",
    "                previous_pr = sum(pr)\n",
    "\n",
    "        # Get weight for each node\n",
    "        node_weight = dict()\n",
    "        for word, index in vocab.items():\n",
    "            node_weight[word] = pr[index]\n",
    "        \n",
    "        self.node_weight = node_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d213d02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = all_text_df['clean'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7572518b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'911 service exist july 28 2022.911[.]re proxy service 2015 sell access hundred thousand microsoft windows computer daily announce week shut wake data breach destroy key component business operation abrupt closure come day krebsonsecurity publish depth look 911 connection shady pay install affiliate program secretly bundle 911 proxy software title include free utility pirate software.911[.]re original residential proxy network allow rent residential ip address use relay internet communication provide anonymity advantage perceive residential user surf web.residential proxy service market people seek ability evade country specific blocking major movie medium streaming provider â\\x80\\x94 like 911 â\\x80\\x94 build network offer â\\x80\\x9cfree vpnâ\\x80\\x9d â\\x80\\x9cfree proxyâ\\x80\\x9d service power software turn userâ\\x80\\x99s pc traffic relay user scenario user use free vpn service unaware turn computer proxy let use internet address transact online.from websiteâ\\x80\\x99s perspective ip traffic residential proxy network user appear originate rent residential ip address proxy service customer service legitimate manner business purpose â\\x80\\x94 price comparison sale intelligence â\\x80\\x94 massively abuse hiding cybercrime activity difficult trace malicious traffic original source.as note krebsonsecurity july 19 story 911 proxy service operate multiple pay install scheme pay affiliate surreptitiously bundle proxy software software continuously generate steady stream new proxy service.a cache copy flashupdate[.]net circa 2016 show homepage pay install affiliate program incentivize silent installation 911 proxy software.within hour story 911 post notice site say review network add series security measure prevent misuse service proxy balance new user registration closed review exist user ensure usage legit compliance term service ”at announcement hell break loose cybercrime forum longtime 911 customer report unable use service affect outage say 911 try implement sort know customer rule maybe 911 try weed customer service high volume cybercriminal activity.then july 28 911 website begin redirect notice say regret inform permanently shut 911 service july 28th ”accorde 911 service hack early july discover manipulate balance large number user account 911 say intruder abuse application programming interface api handle topping account user financial deposit service “not sure hacker 911 message read urgently shut recharge system new user registration investigation start ”the part message 911 user post homepage july 28 2022.however intruder get 911 say manage overwrite critical 911[.]re server datum backup data “on july 28th large number user report log system statement continue find datum server maliciously damage hacker result loss datum backup sic confirm recharge system hack way force difficult decision loss important datum service unrecoverable ”operated largely china 911 enormously popular service cybercrime forum akin critical infrastructure community 911 longtime competitor malware base proxy service vip72 luxsock close door past year.now crime forum rely 911 operation wonder aloud alternative match scale utility 911 offer consensus resounding ”i’m guess soon learn security incident cause 911 implode proxy service spring meet appear burgeon demand service moment comparatively little supply.in meantime 911 absence coincide measurable short live reprieve unwanted traffic internet destination include bank retailer cryptocurrency platform customer proxy service scramble alternative arrangements.riley kilmer co founder proxy tracking service spur.us say 911 network difficult replicate short run “my speculation 911 remain competitor go major boost short term new player eventually come kilmer say good replacement luxsock 911 allow use fraud rate attempt continue replacement service easy monitor stop 911 clean ip address ”911 major proxy provider disclose breach week tie unauthenticated apis july 28 krebsonsecurity report internal apis expose web leak customer database microleave proxy service rotate customer ip address minute investigation show microleave like 911 long history pay install scheme spread proxy software \\r\\n          entry post friday 29th july 2022 03:34 pm \\r\\n          case /actual/ hactivism   love post mortem happened.congrat brian good fight!â\\x80\\x9cnone good replacement l* 9 allow use fraud rate attempt continue replacement service easy monitor stop 911 clean ip addresses.â\\x80\\x9d   clean ip address come time   like ms switch macro make malware container circumvent motw 6 half dozen   proportional dip overall throughput crest   eventually = 30 90 days?perhap rival gang nation state 911 infrastructure purpose   happen personal computer run 911 software?well believe service unrecoverable look easy way thing difficult â\\x80\\x9cwe hack datum lose fault sorryâ\\x80\\x9d make total sense clean seriously highly doubt mean recover services.exactly.where similar proxy?have check prisons?are able help similar 911 s5?prob future daily proxies“the cost far ability bail take easy way credit account doubt refund anyone.refund exist?so trick people instal malware add computer basically ip address laundering ring leave people vulnerable criminal charge company customer use people computer cybercrime good riddance.i’ve notice significant reduction spam proxy service shut down.i get spam email 4 years.less volume high quality.my computer work smoothly humming heat past week \\n unwilling proxy?window try install update entire time.if service operate territory rule ccp know 99 likely spyware highly probable datum route directly ccp intelligence collection service see word surreptitious install china article know right away go mitm datum harvesting op.if youâ\\x80\\x99re service operate territory rule russia know itâ\\x80\\x99s 99 likely malware itâ\\x80\\x99s highly probable datum route directly svr intelligence collection services.the problem know thing actually operate   know easy.your email address publish require field mark comment email website  \\n\\n δdocument.getelementbyid ak_js_1 .setattribute value new date .gettime mail listsearch krebsonsecurityrecent postsspam nationa new york times bestseller think cybersecurity career?read this.all skimmer click image skimmer series.story categoriesthe value hack pcbadguy use pcbadguy use email email account worth far imagine.most popular postswhy hacker hail russiacategory web fraud 2.0innovation undergroundid protection service examinedis antivirus dead?the reason declinethe grow tax fraud menacefile them bad guy inside card shopa crash course carding beware social security fraudsign sign card stolen?finde easy krebs 3 rule online safety'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d067bd3b",
   "metadata": {},
   "source": [
    "# Kyword Extraction with TextRank4zh Package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cb16b",
   "metadata": {},
   "source": [
    "### List of Highest Rank Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "734b8639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service - 12.568874369575667\n",
      "proxy - 7.774293133718644\n",
      "user - 4.582701853755747\n",
      "july - 4.156244340410796\n",
      "datum - 3.8030717986780562\n",
      "customer - 3.5205355391649467\n",
      "use - 3.418926608367013\n",
      "new - 3.2606475059374556\n",
      "computer - 3.2460013267892367\n",
      "address - 3.238942695241574\n",
      "know - 2.727081662734016\n",
      "ip - 2.695437848149397\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text,window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c6b20",
   "metadata": {},
   "source": [
    "### List of  Highest Rank for Verbs and Nouns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6b0a14fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service - 10.834133659781616\n",
      "proxy - 6.59074414614043\n",
      "user - 4.4146032062204705\n",
      "customer - 3.4240055168016377\n",
      "use - 3.220025705926591\n",
      "address - 3.1488852473126223\n",
      "know - 2.5654090116867048\n",
      "ip - 2.4780868894906845\n",
      "network - 2.4721863917954887\n",
      "computer - 2.467033602628897\n",
      "datum - 2.4632980722888553\n",
      "post - 2.2517028185806245\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos=['NOUN', 'VERB'],window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f9c3a6",
   "metadata": {},
   "source": [
    "### List of  Verbs  Based on Highest Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f476b419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use - 2.5346879217859684\n",
      "shut - 2.2684992543625353\n",
      "come - 2.1427993742850515\n",
      "continue - 1.9514885004559481\n",
      "allow - 1.8398685206094059\n",
      "operate - 1.627417509766468\n",
      "add - 1.6120277002125958\n",
      "pay - 1.51246369595393\n",
      "include - 1.4798183123637552\n",
      "bundle - 1.4081900028260708\n",
      "appear - 1.3698190288299665\n",
      "exist - 1.2694248386644218\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos=['VERB'],window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512f3b2f",
   "metadata": {},
   "source": [
    "### List of  Nouns  Based on Highest Rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7cfa5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "service - 10.80839185013926\n",
      "proxy - 6.277617883516081\n",
      "user - 4.066233068380505\n",
      "customer - 3.5126879755290377\n",
      "address - 3.442055613108046\n",
      "computer - 2.4577695350030835\n",
      "network - 2.4076994187182663\n",
      "datum - 2.3898178155921483\n",
      "email - 2.3468984399696864\n",
      "post - 2.254806751522199\n",
      "use - 2.167327136755958\n",
      "install - 2.1544345238068563\n"
     ]
    }
   ],
   "source": [
    "tr4w = TextRank4Keyword()\n",
    "tr4w.analyze(text, candidate_pos=['NOUN'],window_size=4, lower=False)\n",
    "tr4w.get_keywords(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72b6124",
   "metadata": {},
   "source": [
    "###  we can see the weight for each node(word) and the most important words can be used as keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91df9630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb0a9f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
